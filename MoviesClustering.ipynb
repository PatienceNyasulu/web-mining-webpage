{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Netflix Movies and TV-shows Segmentation\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'netflix-shows:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F434238%2F2654038%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240425%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240425T012108Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D819f2d8328b879125c453af2435ba786e9cab8473ec417fe8ab46843a4f4ebfcfc5ba499d4c3cd100e0142084dc98e66efacf26ff57bc4f41c7b5b24c0ddcbe3635e400653802e0e11432be327b8680569d488ff598e8d40e2ac30fc610450bc28033cdceeb8294bee3a43159824f74a4a653e94dd191b064eee229ae65007fdbf469ea980e06f0b42751e72e339f935fa3c30627792d73cbf22c3d06aedced6fd63905a74fb296fd8d937c1ffb7d63b3cdf89656f945790076fc4939f624a2111a1c375987886305d61fd32adae48b5d07182ab5c447675c0af00cd27812963e5f187ca10c0ea1cfb984c7d840b03c2f5a8d1a077bf8a9ee744fb73af28138f'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "xUmLHL_S16mV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e437d598-134f-4cc2-f812-40b8936567ae"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to load (likely expired) https://storage.googleapis.com/kaggle-data-sets/434238/2654038/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240425%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240425T012108Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=819f2d8328b879125c453af2435ba786e9cab8473ec417fe8ab46843a4f4ebfcfc5ba499d4c3cd100e0142084dc98e66efacf26ff57bc4f41c7b5b24c0ddcbe3635e400653802e0e11432be327b8680569d488ff598e8d40e2ac30fc610450bc28033cdceeb8294bee3a43159824f74a4a653e94dd191b064eee229ae65007fdbf469ea980e06f0b42751e72e339f935fa3c30627792d73cbf22c3d06aedced6fd63905a74fb296fd8d937c1ffb7d63b3cdf89656f945790076fc4939f624a2111a1c375987886305d61fd32adae48b5d07182ab5c447675c0af00cd27812963e5f187ca10c0ea1cfb984c7d840b03c2f5a8d1a077bf8a9ee744fb73af28138f to path /kaggle/input/netflix-shows\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Netflix Movies and TV-shows Segmentation is the the unsupervised Machine Learning problem. In this problem by goal is to first make the usable for analysing and clustering process. The first step I did after importing the dataset is knowing the basic information about the dataset. After that I planned the Data Wrangling part and Data preprocessing steps. First I converted a date feature is by default loaded as the object to pandas datetime object so that I can use that feature easily for future purpose as datetime object gives many predefined methods to specifically work with date.\n",
        "\n",
        "Next I didn't handled the missing values in the dataset, because the missing data in appeared in some features with less in numbers so we can use that for EDA which doesn't make any difference. Next thing is I going to use the text based feature that is \"Description\" for training machine learning model. These are the reason behind of not handling the missing values.\n",
        "\n",
        "The next step is text preprocessing, before making this feature into meaningful multi-dimensional vector, I need to do some preprocessing steps that is changing to lowercase, then removed punctual, stopping words and extra white spaces, etc..\n",
        "\n",
        "Finally,\n",
        "I used the TfidVector to change the text data into  numerical,I chose 400 as the maximum features so each review observation will be converted into 400 length features.\n",
        "\n",
        "Then after converting this vector the next step is building clustering machine learning models. I tried three different models, the first one is K-means, then I tried Hierarchical Clustering and finally DBSCAN algorithm.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/balaji-89/Netflix_Movies_Segmentation\n"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine.\n",
        "We need to do:\n",
        "\n",
        "Exploratory Data Analysis\n",
        "\n",
        "Understanding what type content is available in different countries\n",
        "\n",
        "Is Netflix has increasingly focusing on TV rather than movies in recent years.\n",
        "\n",
        "Clustering similar content by matching text-based features"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# Import Libraries\n",
        "# Import Libraries# Import Libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import warnings\n",
        "\n",
        "\n",
        "#text preprocessing libraries\n",
        "import contractions\n",
        "import string\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.base import BaseEstimator,TransformerMixin\n",
        "from sklearn.compose import make_column_transformer\n",
        "\n",
        "\n",
        "#avoid warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "\n",
        "\n",
        "def load_data(path):\n",
        "    '''\n",
        "    Description : It featches the csv data from the specified location\n",
        "                  and returns it as Dataframe object\n",
        "\n",
        "    Parameters :\n",
        "      'path' - Holding path of the xlsx data located.\n",
        "    '''\n",
        "    return pd.read_csv(path)\n",
        "\n",
        "\n",
        "#loading data\n",
        "df = load_data('../Dataset/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy = df.copy(deep = True)"
      ],
      "metadata": {
        "id": "otgh14UR16mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(\"Total Rows: \", df.shape[0])\n",
        "print(\"Total Columns: \", df.shape[1])"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated(keep = 'first').sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "\n",
        "\n",
        "df.isnull().sum().plot(kind= 'bar')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel(\"Total Number of NaN values\")\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a Unsupervised machine project which means we don't have any target variable. This dataset contains 12 features and 7787 observations. Initially it had some duplicated observations and I removed that.\n",
        "This dataset contains movies and series details. And it has some categorical, Text (Description), and Numbers too. Finally there is four features with NaN values director, cast, country and date_added"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Categorical Dataset Describe\n",
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "mVCg1K7v16mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "show_id : Unique ID and it is not useful much.\n",
        "type : This describes whether the particular observation is series or a movie\n",
        "title : This is the title of the movie or show\n",
        "director : Name of the director\n",
        "cast : cast information\n",
        "country: Name of the country where the movie/series are belongs to.\n",
        "date_added : Date is added in Netflix\n",
        "release_year : the date it was actually released.\n",
        "rating : Tv rating of the show\n",
        ".....\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "\n",
        "#Calculaing unique values for most continous cols might not give any insight. Sometime I may want to display\n",
        "#unique values of only catgorical/object dtypes so I created function here.\n",
        "\n",
        "def unique_counts(df1):\n",
        "   for i in df1.columns:\n",
        "       count = df1[i].nunique()\n",
        "       print(i, \": \", count)\n",
        "\n"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#printing unique value of only object cols\n",
        "unique_counts(df)"
      ],
      "metadata": {
        "id": "i9Ar9ZCC16mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***3. Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "8ALl0i0k16m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(5)"
      ],
      "metadata": {
        "id": "eZbkl-ib16m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "#changing date_added feature into pandas datetime\n",
        "\n",
        "def handle_date_added_feature(date_added_values):\n",
        "    fin_date = []\n",
        "    for date in date_added_values:\n",
        "        if pd.isna(date):\n",
        "            fin_date.append(np.nan)\n",
        "        else:\n",
        "            #extracting day\n",
        "            day = date.split()[1]\n",
        "            day = int(day[:-1])\n",
        "            #extracting month\n",
        "            month = date.split()[0]\n",
        "            month_map = {'January':1,'February':2,'March':3,'April':4,'May':5,'June':6,'July':7,'August':8,'September':9,'October':10,'November':11,'December':12}\n",
        "            month =  month_map[month]\n",
        "            #extracting year\n",
        "            year = date.split()[-1]\n",
        "            fin_date.append(f'{year}-{month}-{day}')\n",
        "    #returning as datetime\n",
        "    return pd.to_datetime(fin_date)\n",
        "\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['date_added'] =  handle_date_added_feature(df.date_added)\n"
      ],
      "metadata": {
        "id": "02FGlVou16m2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['cast'] = df['cast'].apply(lambda x : np.nan if pd.isna(x) else x.split(','))"
      ],
      "metadata": {
        "id": "pXa13kl-16m2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is unsupervised problem, so we going to use most of the features for EDA and our goal is to cluster based on the text features and I not going to spend much time in imputing Missing values and this wrangling I converted the date_added feature to pandas dataframe to use that feature effectively. Finally I changed the listed_in and cast to list so that we can use that in EDA little more easily."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "tv_show = df[df.type == 'TV Show']\n",
        "movie = df[df.type == 'Movie']\n",
        "\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1,2)\n",
        "tv_show.director.value_counts()[:15].plot(kind='bar',ax = ax1,title='Top 15 TV Show Directors',figsize = (20,8))\n",
        "movie.director.value_counts()[:15].plot(kind='bar',ax =ax2, title = 'Top 15 Movie Directors',figsize = (20,8))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####  What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alastair Fothergill directed most TV shows in our dataset with total count of 3 TV Shows. Raul Campos is the directed most films in our Movie category with the total movie count of 18."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "tv_show_cst = []\n",
        "for obs in tv_show.iterrows():\n",
        "    if type(obs[1]['cast']) is list:\n",
        "        tv_show_cst.extend(obs[1]['cast'])\n",
        "\n",
        "movie_cst = []\n",
        "for obs in movie.iterrows():\n",
        "    if type(obs[1]['cast']) is list:\n",
        "        movie_cst.extend(obs[1]['cast'])\n",
        "\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1,2)\n",
        "pd.Series(tv_show_cst).value_counts()[:15].plot(kind='bar',ax = ax1,title='Top 15 TV Show Actors',figsize = (20,8))\n",
        "pd.Series(movie_cst).value_counts()[:15].plot(kind='bar',ax =ax2, title = 'Top 15 Movie Actors',figsize = (20,8))\n",
        "plt.show()\n",
        "\n",
        "del tv_show_cst,movie_cst\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Takahiro Sakurai acted most TV shows in our dataset with total count of 25 TV Shows. Anupam Kher is the acted most films in our Movie category with the total movie count of 32."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "\n",
        "#Checking the distribution of Movie Durations\n",
        "plt.figure(figsize=(15,15))\n",
        "\n",
        "plt.subplot(2,2,1)\n",
        "sns.distplot(movie['duration'].str.extract('(\\d+)'),kde=False)\n",
        "plt.title('Distplot with Normal distribution for Movies',fontweight=\"bold\")\n",
        "\n",
        "#Checking the distribution of TV SHOWS\n",
        "plt.subplot(2,1,2)\n",
        "plt.title(\"Distribution of TV Shows duration\",fontweight='bold')\n",
        "sns.countplot(x=tv_show['duration'],data=tv_show,order = tv_show['duration'].value_counts().index)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most TV shows in our dataset is released in only one season almost 1600 and Most films released, has 800 minutes duration and it is normaly distributed."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "\n",
        "\n",
        "#Checking the distribution of TV SHOWS\n",
        "df_word_cloud = tv_show['country'].dropna()\n",
        "text = \" \".join([word.strip().replace(' ','_') for word in df_word_cloud])\n",
        "wordcloud = WordCloud(background_color=\"black\").generate(text)\n",
        "# Display the generated image:\n",
        "# the matplotlib way:\n",
        "plt.figure(figsize=(12,10))\n",
        "plt.imshow(wordcloud, interpolation='blackman')\n",
        "plt.axis(\"off\")\n",
        "plt.title('TV_Show Country', fontsize=18, fontweight='bold')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_word_cloud = movie['country'].dropna()\n",
        "text = \" \".join([word.strip().replace(' ','_') for word in df_word_cloud])\n",
        "wordcloud = WordCloud(background_color=\"black\").generate(text)\n",
        "# Display the generated image:\n",
        "# the matplotlib way:\n",
        "plt.figure(figsize=(12,10))\n",
        "plt.imshow(wordcloud, interpolation='blackman')\n",
        "plt.axis(\"off\")\n",
        "plt.title('Movie Country', fontsize=18, fontweight='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D4-YiIPP16m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Netflix has more number of TV_shows and Movies belongs to United States, we could guess this easily as Netflix belongs to America so It would be their first market preference."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "\n",
        "# Top 10 Genre in movies\n",
        "plt.figure(figsize=(18,6))\n",
        "sns.barplot(x = df[\"listed_in\"].value_counts().head(15).index,\n",
        "            y = df[\"listed_in\"].value_counts().head(15).values)\n",
        "plt.xticks(rotation=80)\n",
        "plt.title(\"Top10 Genre in Movies\",size='16',fontweight=\"bold\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Documentaries, Stand_up comedy and Dramas,International Movies is very famous and most contents available in Netflix has same genre."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "\n",
        "# visualizing the movies and tv_shows based on the release year\n",
        "\n",
        "\n",
        "movies_year =movie['release_year'].value_counts().sort_index(ascending=False)\n",
        "tvshows_year =tv_show['release_year'].value_counts().sort_index(ascending=False)\n",
        "\n",
        "\n",
        "sns.set(font_scale=1.4)\n",
        "movies_year.plot(figsize=(12, 8), linewidth=2.5, label=\"Movies / year\")\n",
        "tvshows_year.plot(figsize=(12, 8), linewidth=2.5,label=\"TV Shows / year\")\n",
        "plt.xlabel(\"Years\", labelpad=15)\n",
        "plt.ylabel(\"Number\", labelpad=15)\n",
        "plt.title(\"Production growth yearly\", y=1.02, fontsize=22)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see that Netflix acheived its peak between 2017 to 2020. This may because of Corona. Because because of corona people stayed in their house which make them to spend more time in internet.So it is clearly understood by Netflix and they make sure to own more contents in that period to attract more subscribers over other OTT platform."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "#Rating based on rating system of all TV Shows\n",
        "\n",
        "tv_ratings = tv_show.groupby(['rating'])['show_id'].count().reset_index(name='count').sort_values(by='count',ascending=False)\n",
        "fig_dims = (14,7)\n",
        "fig, ax = plt.subplots(figsize=fig_dims)\n",
        "sns.pointplot(x='rating',y='count',data=tv_ratings)\n",
        "plt.title('TV Show Ratings',size='20')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TV-MA rating category content is more available in Netflix and followed by TV-14"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Chart - 8 visualization code\n",
        "# Count of TVshow and Movie produced in different country\n",
        "\n",
        "\n",
        "df_country = df.groupby(['country', 'type'])['show_id'].count().sort_values(ascending = False).reset_index()\n",
        "plt.figure(figsize = (15, 6))\n",
        "sns.barplot(data = df_country, x = df_country['country'][:20], y = df_country['show_id'], hue = 'type')\n",
        "plt.xticks(rotation = 90)\n",
        "plt.legend(loc = 'upper right')\n",
        "plt.ylabel('Total Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we seen in the word cloud Netflix has more number of United States's Tv shows and Movies. Followed by India, which has highest number of movies and very low number of TV_shows comparing to the others."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***5. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Description is the only features which I need for training ML model, and I don't have any missing values in the Description feature so there is no need to handle impute any missing values."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I going to handle only text data here so, I am not going to taking care of other variables here."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#decoding non-utf-8 characters\n",
        "def remove_non_utf8_words(df,features_names):\n",
        "    df = df.copy()\n",
        "    for feature in features_names:\n",
        "        df[feature] = df[feature].apply(lambda x : x.replace('â€™',\"'\"))\n",
        "        df[feature] = df[feature].apply(lambda x : ''.join([c for c in x if ord(c) < 128]))\n",
        "    return df"
      ],
      "metadata": {
        "id": "7wRRnB9c16nP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "def expand_contractions(df,feature_names):\n",
        "    df = df.copy()\n",
        "    for feature in feature_names:\n",
        "        df[feature] = df[feature].apply(lambda x : \" \".join(x.split()))\n",
        "        df[feature] = df[feature].apply(lambda x : contractions.fix(x))\n",
        "    return df\n",
        "\n"
      ],
      "metadata": {
        "id": "5VPvjgtV16nR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "def change_to_lower_case(df,feature_names):\n",
        "    df = df.copy()\n",
        "    for feature in feature_names:\n",
        "        df[feature] = df[feature].apply(lambda x : x.lower())\n",
        "    return df"
      ],
      "metadata": {
        "id": "rz1VvKdT16nR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "def remove_punctuations(df,features_columns):\n",
        "    df = df.copy()\n",
        "    punctuations = string.punctuation\n",
        "    for feature in features_columns:\n",
        "        df[feature] = df[feature].apply(lambda x : x.translate(str.maketrans('','',punctuations)))\n",
        "    return df"
      ],
      "metadata": {
        "id": "QjsLfI_k16nS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "def remove_urls(df,feature_names):\n",
        "    df = df.copy()\n",
        "    for feature in feature_names:\n",
        "        df[feature] = df[feature].apply(lambda x :  re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', x))\n",
        "    return df\n",
        "\n",
        "def remove_words_with_digits(df, feature_names):\n",
        "    df = df.copy()\n",
        "    for feature in feature_names:\n",
        "        df[feature] = df[feature].apply(lambda x : \" \".join(s for s in x.split() if not any(c.isdigit() for c in s)))\n",
        "    return df"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "def remove_stopwords(df,features_names):\n",
        "    df = df.copy()\n",
        "    eng_stopwords = set(stopwords.words('english'))\n",
        "    for feature in features_names:\n",
        "        df[feature] = df[feature].apply(lambda text: \" \".join(word for word in text.split() if not word in eng_stopwords))\n",
        "    return df"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Tokenization and Text Normalization"
      ],
      "metadata": {
        "id": "O8BgTOVm16nU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "def tokenize_and_normalization(df,feature_names):\n",
        "    lemmatizer=WordNetLemmatizer()\n",
        "    for feature in feature_names:\n",
        "            df[feature] = df[feature].apply(lambda x :\" \".join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "koZcyPXL16nU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "gSXlH1ni16nV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I using Lemmatization normalization because it will do better than stemming. In stemming, there is chance that it will change the word completely. But in case of Lemmatization, it is not the case, it will try to maintain the original context of the sentence."
      ],
      "metadata": {
        "id": "uZxyhhCQ16nW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Text Vectorization"
      ],
      "metadata": {
        "id": "FzXVNbBt16nW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTextDataPreprocessing(BaseEstimator,TransformerMixin):\n",
        "\n",
        "    def __init__(self,feature_names):\n",
        "        self.feature_names = feature_names\n",
        "        return None\n",
        "\n",
        "    #decoding non-utf-8 characters\n",
        "    def remove_non_utf8_words(self,df,feature_names):\n",
        "        df = df.copy()\n",
        "        for feature in feature_names:\n",
        "            df[feature] = df[feature].apply(lambda x : x.replace('â€™',\"'\"))\n",
        "            df[feature] = df[feature].apply(lambda x : ''.join([c for c in x if ord(c) < 128]))\n",
        "        return df\n",
        "\n",
        "    # Expand Contraction\n",
        "    def expand_contractions(self,df,feature_names):\n",
        "        df = df.copy()\n",
        "        for feature in feature_names:\n",
        "            df[feature] = df[feature].apply(lambda x : \" \".join(x.split()))\n",
        "            df[feature] = df[feature].apply(lambda x : contractions.fix(x))\n",
        "        return df\n",
        "\n",
        "        # Lower Casing\n",
        "    def change_to_lower_case(self,df,feature_names):\n",
        "        df = df.copy()\n",
        "        for feature in feature_names:\n",
        "            df[feature] = df[feature].apply(lambda x : x.lower())\n",
        "        return df\n",
        "\n",
        "        # Remove Punctuations\n",
        "    def remove_punctuations(self,df,features_names):\n",
        "        df = df.copy()\n",
        "        punctuations = string.punctuation\n",
        "        for feature in features_names:\n",
        "            df[feature] = df[feature].apply(lambda x : x.translate(str.maketrans('','',punctuations)))\n",
        "        return df\n",
        "\n",
        "    # Remove URLs & Remove words and digits contain digits\n",
        "    def remove_urls(self,df,feature_names):\n",
        "        df = df.copy()\n",
        "        for feature in feature_names:\n",
        "            df[feature] = df[feature].apply(lambda x :  re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', x))\n",
        "        return df\n",
        "\n",
        "    def remove_words_with_digits(self,df, feature_names):\n",
        "        df = df.copy()\n",
        "        for feature in feature_names:\n",
        "            df[feature] = df[feature].apply(lambda x : \" \".join(s for s in x.split() if not any(c.isdigit() for c in s)))\n",
        "        return df\n",
        "\n",
        "    # Remove Stopwords\n",
        "    def remove_stopwords(self,df,feature_names):\n",
        "        df = df.copy()\n",
        "        eng_stopwords = set(stopwords.words('english'))\n",
        "        for feature in feature_names:\n",
        "            df[feature] = df[feature].apply(lambda text: \" \".join(word for word in text.split() if not word in eng_stopwords))\n",
        "        return df\n",
        "\n",
        "    # Tokenization\n",
        "    def tokenize_and_normalization(self,df,feature_names):\n",
        "        lemmatizer=WordNetLemmatizer()\n",
        "        for feature in feature_names:\n",
        "            df[feature] = df[feature].apply(lambda x :\" \".join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "    def fit(self,df):\n",
        "        return self\n",
        "\n",
        "    def transform(self,df):\n",
        "        df =df.copy()\n",
        "        #removing non utf8 words\n",
        "        df = self.remove_non_utf8_words(df,self.feature_names)\n",
        "        #expanding contractions\n",
        "        df = self.expand_contractions(df,self.feature_names)\n",
        "        #changing all to lower case\n",
        "        df = self.change_to_lower_case(df,self.feature_names)\n",
        "        #remvoing punctuations\n",
        "        df = self.remove_punctuations(df,self.feature_names)\n",
        "        #removing urls\n",
        "        df = self.remove_urls(df,self.feature_names)\n",
        "        #removing words with digits\n",
        "        df = self.remove_words_with_digits(df,self.feature_names)\n",
        "        #remove stopwords\n",
        "        df = self.remove_stopwords(df,self.feature_names)\n",
        "        #remove tokenize and normalization\n",
        "        df = self.tokenize_and_normalization(df,self.feature_names)\n",
        "\n",
        "\n",
        "        return df\n"
      ],
      "metadata": {
        "id": "RjdQx2yy16nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "class CustomTfidVectorizer(BaseEstimator,TransformerMixin):\n",
        "    def __init__(self,feature_name,max_features = None):\n",
        "        self.max_features = max_features\n",
        "        self.feature_name = feature_name\n",
        "        return None\n",
        "\n",
        "    def fit(self,df):\n",
        "        self.TfidVectorizer = TfidfVectorizer(max_features= self.max_features)\n",
        "        self.TfidVectorizer.fit(df[self.feature_name])\n",
        "        return self\n",
        "\n",
        "    def transform(self,df):\n",
        "        df = df.copy()\n",
        "        vectors = self.TfidVectorizer.transform(df[self.feature_name]).toarray()\n",
        "        df[self.TfidVectorizer.get_feature_names()] = vectors\n",
        "        df.drop(self.feature_name,axis = 1,inplace = True)\n",
        "        return df"
      ],
      "metadata": {
        "id": "_-0dkVKH16nY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "NSQK2V2c16nY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Term frequency-inverse document frequency ( TF-IDF) gives a measure that takes the importance of a word into consideration depending on how frequently it occurs in a document and a corpus.Not like Bag of words and Count Vector technique which treats all words equally,TF-IDF can distinguish very common words or rare words"
      ],
      "metadata": {
        "id": "nxaMGLSU16nZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***6. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#using pipeline to transform our text data\n",
        "text_feature_pipeline = Pipeline([\n",
        "    ('text_preprocessing',CustomTextDataPreprocessing(feature_names=['description'])),\n",
        "    ('vectorization',CustomTfidVectorizer(feature_name='description',max_features=400))\n",
        "])\n"
      ],
      "metadata": {
        "id": "7RknnMmU16na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "description_feature_vector = text_feature_pipeline.fit_transform(df).iloc[:,11:]"
      ],
      "metadata": {
        "id": "QVMvOiiP16na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#printing the shapes of our data\n",
        "print(\"Train data: \",description_feature_vector.shape)"
      ],
      "metadata": {
        "id": "KjfsfuOG16na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 10"
      ],
      "metadata": {
        "id": "JCxE_2Hk16nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "#finding optimal number of clusters using the elbow method\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "ssd= []\n",
        "\n",
        "#Using for loop for iterations from 1 to 30.\n",
        "for cluster in range(2, 15):\n",
        "    kmeans = KMeans(n_clusters=cluster,random_state= SEED)\n",
        "    kmeans.fit(description_feature_vector)\n",
        "    preds = kmeans.predict(description_feature_vector)\n",
        "    score = silhouette_score(description_feature_vector, preds)\n",
        "    print(\"For n_clusters = {}, Silhouette score is {}\".format(cluster, score))\n",
        "    ssd.append(kmeans.inertia_)\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "plt.plot(range(2, 15), ssd)\n",
        "plt.xticks(range(2,15))\n",
        "plt.title('The Elbow Method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Sum of Squared distance')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above Elbow method and silhouette score I am choosing 12 would be the perfect number of clusters for this problem. So I am finally training kmeans with 12 clusters."
      ],
      "metadata": {
        "id": "y3tFvQbc16nc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training the K-means model on a dataset\n",
        "kmeans = KMeans(n_clusters= 12, init='k-means++', random_state= SEED)\n",
        "y_predict= kmeans.fit_predict(description_feature_vector)"
      ],
      "metadata": {
        "id": "juK1n0fA16nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict the clusters and evaluate the silhouette score\n",
        "score = silhouette_score(description_feature_vector, y_predict)\n",
        "print(\"Silhouette score is {}\".format(score))"
      ],
      "metadata": {
        "id": "bCpELtka16nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Reducing the number of features to visualize it in 2D or 3D plot\n",
        "pca = PCA(n_components = 3)\n",
        "X = pca.fit_transform(description_feature_vector)\n"
      ],
      "metadata": {
        "id": "2J66HJ0i16nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "plt.title('customer segmentation based on Recency and Monetary')\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_predict, s=50)\n",
        "\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BKrlOtrF16nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting 3D Graph\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "fig=plt.figure(figsize=(15,10))\n",
        "plt.title('3d visualization of Recency Frequency and Monetary')\n",
        "ax=fig.add_subplot(111,projection='3d')\n",
        "xs=X[:,0]\n",
        "ys=X[:,1]\n",
        "zs=X[:,2]\n",
        "ax.scatter(xs,ys,zs,s=5,c = y_predict)\n",
        "ax.set_xlabel('PCA 0')\n",
        "ax.set_ylabel('PCA 1')\n",
        "ax.set_zlabel('PCA 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M_gbKEim16nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the end, from the 2D and 3D plot we can say that K-means done not a bad job, but we cannot get into a conclusion by plotting this PCA features."
      ],
      "metadata": {
        "id": "niN6Oyy716ne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the dendogram to find the optimal number of clusters\n",
        "import scipy.cluster.hierarchy as sch\n",
        "plt.figure(figsize=(13,8))\n",
        "dendrogram = sch.dendrogram(sch.linkage(description_feature_vector, method = 'ward'))\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Customers')\n",
        "plt.ylabel('Euclidean Distances')\n",
        "plt.show() # find largest vertical distance we can make without crossing any other horizontal line"
      ],
      "metadata": {
        "id": "83XRnK0o16nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "aggh = AgglomerativeClustering(n_clusters= 9, affinity='euclidean', linkage='ward')\n",
        "aggh.fit(description_feature_vector)\n",
        "#Predicting using our model\n",
        "y_hc=aggh.fit_predict(description_feature_vector)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Silhouette Coefficient\n",
        "print(\"Silhouette Coefficient: %0.3f\"%silhouette_score(description_feature_vector,y_hc, metric='euclidean'))"
      ],
      "metadata": {
        "id": "w7A8b8sf16ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "d3ySUFTJ16ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model 3\n",
        "\n",
        "#finding optimal number of clusters using the elbow method\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "eps_range=range(6,12) # note, we will scale this down by 100 as we want to explore 0.06 - 0.11 range\n",
        "minpts_range=range(5,14)\n",
        "\n",
        "silhouette_scores= []\n",
        "comb = []\n",
        "\n",
        "#Using for loop for iterations from 1 to 30.\n",
        "for k in eps_range:\n",
        "    for j in minpts_range:\n",
        "        # Set the model and its parameters+\n",
        "        model = DBSCAN(eps=k/100, min_samples=j)\n",
        "        # Fit the model\n",
        "        clm = model.fit(description_feature_vector)\n",
        "        # Calculate Silhoutte Score and append to a list\n",
        "        silhouette_scores.append(silhouette_score(description_feature_vector, clm.labels_, metric='euclidean'))\n",
        "        comb.append(str(k)+\"|\"+str(j)) # axis values for the graph"
      ],
      "metadata": {
        "id": "Wc6E17IJ16nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the resulting Silhouette scores on a graph\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.plot(comb, silhouette_scores, 'bo-')\n",
        "plt.xlabel('Epsilon/100 | MinPts')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Score based on different combnation of Hyperparameters')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TGG9tZwb16nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the above, we can see that eps=0.08 produce the highest scores.\n",
        "A few combinations ended up having very similar scores, indicating that the clustering output for those combinations would also be similar."
      ],
      "metadata": {
        "id": "c5nhHRZj16nh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_model = DBSCAN(eps=6/100, min_samples=5)\n",
        "clm = model.fit(description_feature_vector)\n",
        "print(silhouette_score(description_feature_vector, clm.labels_, metric='euclidean'))\n"
      ],
      "metadata": {
        "id": "AafJ5CcA16nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X[:,0], X[:,1], c= clm.labels_)"
      ],
      "metadata": {
        "id": "dM7YsJYm16ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Silhouette score helped us to find the best model. And helped us to choose the best model among these."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Means did better job in this dataset. It gives the insight that 12 is the perfect number of cluster, but Incase of Aggloramative Clustering  we saw that the 9 is the optimal cluster.Might be 9 is better, but I chose 12 in K-means, I got  a better result with that using k-means. And DBSCAN doen't perform well in this dataset."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***7.*** ***Saving Best Model***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "id": "BQvhMiRDQnPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st"
      ],
      "metadata": {
        "id": "ezooZMlzQqIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display clustered movies\n",
        "st.title('Movie Clustering')\n",
        "\n",
        "for cluster_id in range(k):\n",
        "    st.subheader(f'Cluster {cluster_id + 1}')\n",
        "    cluster_movies = df[df['cluster'] == cluster_id]['title'].tolist()\n",
        "    for movie in cluster_movies:\n",
        "        st.write(movie)"
      ],
      "metadata": {
        "id": "X9sWfW-RLY1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Streamlit web page\n",
        "st.title('Netflix Movies Clustering')\n",
        "\n",
        "# Display movies by cluster\n",
        "unique_clusters = sorted(list(set(y_predict)))\n",
        "for cluster in unique_clusters:\n",
        "    cluster_movies = df[y_predict == cluster]['Title']\n",
        "    st.write(f'Cluster {cluster + 1}')\n",
        "    st.write(cluster_movies)"
      ],
      "metadata": {
        "id": "RWkgsnidQRm1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}