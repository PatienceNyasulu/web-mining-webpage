# -*- coding: utf-8 -*-
"""MoviesClustering

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U_4qMJEjiUHWxMYscbqNlTEjoqYrCTb5

# **Project Name**    - Netflix Movies and TV-shows Segmentation
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'netflix-shows:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F434238%2F2654038%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240425%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240425T012108Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D819f2d8328b879125c453af2435ba786e9cab8473ec417fe8ab46843a4f4ebfcfc5ba499d4c3cd100e0142084dc98e66efacf26ff57bc4f41c7b5b24c0ddcbe3635e400653802e0e11432be327b8680569d488ff598e8d40e2ac30fc610450bc28033cdceeb8294bee3a43159824f74a4a653e94dd191b064eee229ae65007fdbf469ea980e06f0b42751e72e339f935fa3c30627792d73cbf22c3d06aedced6fd63905a74fb296fd8d937c1ffb7d63b3cdf89656f945790076fc4939f624a2111a1c375987886305d61fd32adae48b5d07182ab5c447675c0af00cd27812963e5f187ca10c0ea1cfb984c7d840b03c2f5a8d1a077bf8a9ee744fb73af28138f'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

"""# **Project Summary -**

Netflix Movies and TV-shows Segmentation is the the unsupervised Machine Learning problem. In this problem by goal is to first make the usable for analysing and clustering process. The first step I did after importing the dataset is knowing the basic information about the dataset. After that I planned the Data Wrangling part and Data preprocessing steps. First I converted a date feature is by default loaded as the object to pandas datetime object so that I can use that feature easily for future purpose as datetime object gives many predefined methods to specifically work with date.

Next I didn't handled the missing values in the dataset, because the missing data in appeared in some features with less in numbers so we can use that for EDA which doesn't make any difference. Next thing is I going to use the text based feature that is "Description" for training machine learning model. These are the reason behind of not handling the missing values.

The next step is text preprocessing, before making this feature into meaningful multi-dimensional vector, I need to do some preprocessing steps that is changing to lowercase, then removed punctual, stopping words and extra white spaces, etc..

Finally,
I used the TfidVector to change the text data into  numerical,I chose 400 as the maximum features so each review observation will be converted into 400 length features.

Then after converting this vector the next step is building clustering machine learning models. I tried three different models, the first one is K-means, then I tried Hierarchical Clustering and finally DBSCAN algorithm.

# **GitHub Link -**

https://github.com/balaji-89/Netflix_Movies_Segmentation

# **Problem Statement**

This dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine.
We need to do:

Exploratory Data Analysis

Understanding what type content is available in different countries

Is Netflix has increasingly focusing on TV rather than movies in recent years.

Clustering similar content by matching text-based features

## ***Let's Begin !***

# ***1. Know Your Data***

### Import Libraries
"""

# Import Libraries
# Import Libraries
# Import Libraries# Import Libraries
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns
import joblib
import warnings


#text preprocessing libraries
import contractions
import string
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.stem import PorterStemmer



from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.base import BaseEstimator,TransformerMixin
from sklearn.compose import make_column_transformer


#avoid warnings
warnings.filterwarnings("ignore")

"""### Dataset Loading"""

# Load Dataset


def load_data(path):
    '''
    Description : It featches the csv data from the specified location
                  and returns it as Dataframe object

    Parameters :
      'path' - Holding path of the xlsx data located.
    '''
    return pd.read_csv(path)


#loading data
df = load_data('../Dataset/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')

"""### Dataset First View"""

df_copy = df.copy(deep = True)

# Dataset First Look
df.head(10)

"""### Dataset Rows & Columns count"""

# Dataset Rows & Columns count
print("Total Rows: ", df.shape[0])
print("Total Columns: ", df.shape[1])

"""### Dataset Information"""

# Dataset Info
df.info()

"""#### Duplicate Values"""

# Dataset Duplicate Value Count
df.duplicated(keep = 'first').sum()

"""#### Missing Values/Null Values"""

# Missing Values/Null Values Count
df.isna().sum()

# Visualizing the missing values


df.isnull().sum().plot(kind= 'bar')
plt.xlabel('Features')
plt.ylabel("Total Number of NaN values")

"""### What did you know about your dataset?

This is a Unsupervised machine project which means we don't have any target variable. This dataset contains 12 features and 7787 observations. Initially it had some duplicated observations and I removed that.
This dataset contains movies and series details. And it has some categorical, Text (Description), and Numbers too. Finally there is four features with NaN values director, cast, country and date_added

# ***2. Understanding Your Variables***
"""

# Dataset Columns
df.columns

#Categorical Dataset Describe
df.describe(include='all')

"""### Variables Description

show_id : Unique ID and it is not useful much.
type : This describes whether the particular observation is series or a movie
title : This is the title of the movie or show
director : Name of the director
cast : cast information
country: Name of the country where the movie/series are belongs to.
date_added : Date is added in Netflix
release_year : the date it was actually released.
rating : Tv rating of the show
.....

### Check Unique Values for each variable.
"""

# Check Unique Values for each variable.

#Calculaing unique values for most continous cols might not give any insight. Sometime I may want to display
#unique values of only catgorical/object dtypes so I created function here.

def unique_counts(df1):
   for i in df1.columns:
       count = df1[i].nunique()
       print(i, ": ", count)

#printing unique value of only object cols
unique_counts(df)

"""# ***3. Data Wrangling***

### Data Wrangling Code
"""

df.isna().sum()

df.sample(5)

# Write your code to make your dataset analysis ready.

#changing date_added feature into pandas datetime

def handle_date_added_feature(date_added_values):
    fin_date = []
    for date in date_added_values:
        if pd.isna(date):
            fin_date.append(np.nan)
        else:
            #extracting day
            day = date.split()[1]
            day = int(day[:-1])
            #extracting month
            month = date.split()[0]
            month_map = {'January':1,'February':2,'March':3,'April':4,'May':5,'June':6,'July':7,'August':8,'September':9,'October':10,'November':11,'December':12}
            month =  month_map[month]
            #extracting year
            year = date.split()[-1]
            fin_date.append(f'{year}-{month}-{day}')
    #returning as datetime
    return pd.to_datetime(fin_date)

df['date_added'] =  handle_date_added_feature(df.date_added)

df['cast'] = df['cast'].apply(lambda x : np.nan if pd.isna(x) else x.split(','))

"""### What all manipulations have you done and insights you found?

This is unsupervised problem, so we going to use most of the features for EDA and our goal is to cluster based on the text features and I not going to spend much time in imputing Missing values and this wrangling I converted the date_added feature to pandas dataframe to use that feature effectively. Finally I changed the listed_in and cast to list so that we can use that in EDA little more easily.

# ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***

#### Chart - 1
"""

# Chart - 1 visualization code
tv_show = df[df.type == 'TV Show']
movie = df[df.type == 'Movie']


fig, (ax1, ax2) = plt.subplots(1,2)
tv_show.director.value_counts()[:15].plot(kind='bar',ax = ax1,title='Top 15 TV Show Directors',figsize = (20,8))
movie.director.value_counts()[:15].plot(kind='bar',ax =ax2, title = 'Top 15 Movie Directors',figsize = (20,8))
plt.show()

"""#####  What is/are the insight(s) found from the chart?

Alastair Fothergill directed most TV shows in our dataset with total count of 3 TV Shows. Raul Campos is the directed most films in our Movie category with the total movie count of 18.

#### Chart - 2
"""

# Chart - 2 visualization code
tv_show_cst = []
for obs in tv_show.iterrows():
    if type(obs[1]['cast']) is list:
        tv_show_cst.extend(obs[1]['cast'])

movie_cst = []
for obs in movie.iterrows():
    if type(obs[1]['cast']) is list:
        movie_cst.extend(obs[1]['cast'])


fig, (ax1, ax2) = plt.subplots(1,2)
pd.Series(tv_show_cst).value_counts()[:15].plot(kind='bar',ax = ax1,title='Top 15 TV Show Actors',figsize = (20,8))
pd.Series(movie_cst).value_counts()[:15].plot(kind='bar',ax =ax2, title = 'Top 15 Movie Actors',figsize = (20,8))
plt.show()

del tv_show_cst,movie_cst

"""##### What is/are the insight(s) found from the chart?

Takahiro Sakurai acted most TV shows in our dataset with total count of 25 TV Shows. Anupam Kher is the acted most films in our Movie category with the total movie count of 32.

#### Chart - 3
"""

# Chart - 3 visualization code

#Checking the distribution of Movie Durations
plt.figure(figsize=(15,15))

plt.subplot(2,2,1)
sns.distplot(movie['duration'].str.extract('(\d+)'),kde=False)
plt.title('Distplot with Normal distribution for Movies',fontweight="bold")

#Checking the distribution of TV SHOWS
plt.subplot(2,1,2)
plt.title("Distribution of TV Shows duration",fontweight='bold')
sns.countplot(x=tv_show['duration'],data=tv_show,order = tv_show['duration'].value_counts().index)
plt.show()

"""##### What is/are the insight(s) found from the chart?

Most TV shows in our dataset is released in only one season almost 1600 and Most films released, has 800 minutes duration and it is normaly distributed.

#### Chart - 4
"""

# Chart - 4 visualization code
from wordcloud import WordCloud, STOPWORDS



#Checking the distribution of TV SHOWS
df_word_cloud = tv_show['country'].dropna()
text = " ".join([word.strip().replace(' ','_') for word in df_word_cloud])
wordcloud = WordCloud(background_color="black").generate(text)
# Display the generated image:
# the matplotlib way:
plt.figure(figsize=(12,10))
plt.imshow(wordcloud, interpolation='blackman')
plt.axis("off")
plt.title('TV_Show Country', fontsize=18, fontweight='bold')
plt.show()

df_word_cloud = movie['country'].dropna()
text = " ".join([word.strip().replace(' ','_') for word in df_word_cloud])
wordcloud = WordCloud(background_color="black").generate(text)
# Display the generated image:
# the matplotlib way:
plt.figure(figsize=(12,10))
plt.imshow(wordcloud, interpolation='blackman')
plt.axis("off")
plt.title('Movie Country', fontsize=18, fontweight='bold')
plt.show()

"""##### What is/are the insight(s) found from the chart?

Netflix has more number of TV_shows and Movies belongs to United States, we could guess this easily as Netflix belongs to America so It would be their first market preference.

#### Chart - 5
"""

# Chart - 5 visualization code

# Top 10 Genre in movies
plt.figure(figsize=(18,6))
sns.barplot(x = df["listed_in"].value_counts().head(15).index,
            y = df["listed_in"].value_counts().head(15).values)
plt.xticks(rotation=80)
plt.title("Top10 Genre in Movies",size='16',fontweight="bold")
plt.show()

"""##### What is/are the insight(s) found from the chart?

The Documentaries, Stand_up comedy and Dramas,International Movies is very famous and most contents available in Netflix has same genre.

#### Chart - 6
"""

# Chart - 6 visualization code

# visualizing the movies and tv_shows based on the release year


movies_year =movie['release_year'].value_counts().sort_index(ascending=False)
tvshows_year =tv_show['release_year'].value_counts().sort_index(ascending=False)


sns.set(font_scale=1.4)
movies_year.plot(figsize=(12, 8), linewidth=2.5, label="Movies / year")
tvshows_year.plot(figsize=(12, 8), linewidth=2.5,label="TV Shows / year")
plt.xlabel("Years", labelpad=15)
plt.ylabel("Number", labelpad=15)
plt.title("Production growth yearly", y=1.02, fontsize=22)
plt.legend()
plt.show()

"""##### What is/are the insight(s) found from the chart?

As we can see that Netflix acheived its peak between 2017 to 2020. This may because of Corona. Because because of corona people stayed in their house which make them to spend more time in internet.So it is clearly understood by Netflix and they make sure to own more contents in that period to attract more subscribers over other OTT platform.

#### Chart - 7
"""

# Chart - 7 visualization code
#Rating based on rating system of all TV Shows

tv_ratings = tv_show.groupby(['rating'])['show_id'].count().reset_index(name='count').sort_values(by='count',ascending=False)
fig_dims = (14,7)
fig, ax = plt.subplots(figsize=fig_dims)
sns.pointplot(x='rating',y='count',data=tv_ratings)
plt.title('TV Show Ratings',size='20')
plt.show()

"""##### What is/are the insight(s) found from the chart?

TV-MA rating category content is more available in Netflix and followed by TV-14

#### Chart - 8
"""

## Chart - 8 visualization code
# Count of TVshow and Movie produced in different country


df_country = df.groupby(['country', 'type'])['show_id'].count().sort_values(ascending = False).reset_index()
plt.figure(figsize = (15, 6))
sns.barplot(data = df_country, x = df_country['country'][:20], y = df_country['show_id'], hue = 'type')
plt.xticks(rotation = 90)
plt.legend(loc = 'upper right')
plt.ylabel('Total Count')
plt.show()

"""##### What is/are the insight(s) found from the chart?

As we seen in the word cloud Netflix has more number of United States's Tv shows and Movies. Followed by India, which has highest number of movies and very low number of TV_shows comparing to the others.

# ***5. Feature Engineering & Data Pre-processing***

### 1. Handling Missing Values

#### What all missing value imputation techniques have you used and why did you use those techniques?

The Description is the only features which I need for training ML model, and I don't have any missing values in the Description feature so there is no need to handle impute any missing values.

### 2. Categorical Encoding

#### What all categorical encoding techniques have you used & why did you use those techniques?

I going to handle only text data here so, I am not going to taking care of other variables here.

### 3. Textual Data Preprocessing
(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)

#### 1. Expand Contraction
"""

#decoding non-utf-8 characters
def remove_non_utf8_words(df,features_names):
    df = df.copy()
    for feature in features_names:
        df[feature] = df[feature].apply(lambda x : x.replace('â€™',"'"))
        df[feature] = df[feature].apply(lambda x : ''.join([c for c in x if ord(c) < 128]))
    return df

# Expand Contraction
def expand_contractions(df,feature_names):
    df = df.copy()
    for feature in feature_names:
        df[feature] = df[feature].apply(lambda x : " ".join(x.split()))
        df[feature] = df[feature].apply(lambda x : contractions.fix(x))
    return df

"""#### 2. Lower Casing"""

# Lower Casing
def change_to_lower_case(df,feature_names):
    df = df.copy()
    for feature in feature_names:
        df[feature] = df[feature].apply(lambda x : x.lower())
    return df

"""#### 3. Removing Punctuations"""

# Remove Punctuations
def remove_punctuations(df,features_columns):
    df = df.copy()
    punctuations = string.punctuation
    for feature in features_columns:
        df[feature] = df[feature].apply(lambda x : x.translate(str.maketrans('','',punctuations)))
    return df

"""#### 4. Removing URLs & Removing words and digits contain digits."""

# Remove URLs & Remove words and digits contain digits
def remove_urls(df,feature_names):
    df = df.copy()
    for feature in feature_names:
        df[feature] = df[feature].apply(lambda x :  re.sub(r'^https?:\/\/.*[\r\n]*', '', x))
    return df

def remove_words_with_digits(df, feature_names):
    df = df.copy()
    for feature in feature_names:
        df[feature] = df[feature].apply(lambda x : " ".join(s for s in x.split() if not any(c.isdigit() for c in s)))
    return df

"""#### 5. Removing Stopwords & Removing White spaces"""

# Remove Stopwords
def remove_stopwords(df,features_names):
    df = df.copy()
    eng_stopwords = set(stopwords.words('english'))
    for feature in features_names:
        df[feature] = df[feature].apply(lambda text: " ".join(word for word in text.split() if not word in eng_stopwords))
    return df

"""#### 6. Tokenization and Text Normalization"""

# Tokenization
def tokenize_and_normalization(df,feature_names):
    lemmatizer=WordNetLemmatizer()
    for feature in feature_names:
            df[feature] = df[feature].apply(lambda x :" ".join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))

    return df

"""##### Which text normalization technique have you used and why?

I using Lemmatization normalization because it will do better than stemming. In stemming, there is chance that it will change the word completely. But in case of Lemmatization, it is not the case, it will try to maintain the original context of the sentence.

#### 7. Text Vectorization
"""

class CustomTextDataPreprocessing(BaseEstimator,TransformerMixin):

    def __init__(self,feature_names):
        self.feature_names = feature_names
        return None

    #decoding non-utf-8 characters
    def remove_non_utf8_words(self,df,feature_names):
        df = df.copy()
        for feature in feature_names:
            df[feature] = df[feature].apply(lambda x : x.replace('â€™',"'"))
            df[feature] = df[feature].apply(lambda x : ''.join([c for c in x if ord(c) < 128]))
        return df

    # Expand Contraction
    def expand_contractions(self,df,feature_names):
        df = df.copy()
        for feature in feature_names:
            df[feature] = df[feature].apply(lambda x : " ".join(x.split()))
            df[feature] = df[feature].apply(lambda x : contractions.fix(x))
        return df

        # Lower Casing
    def change_to_lower_case(self,df,feature_names):
        df = df.copy()
        for feature in feature_names:
            df[feature] = df[feature].apply(lambda x : x.lower())
        return df

        # Remove Punctuations
    def remove_punctuations(self,df,features_names):
        df = df.copy()
        punctuations = string.punctuation
        for feature in features_names:
            df[feature] = df[feature].apply(lambda x : x.translate(str.maketrans('','',punctuations)))
        return df

    # Remove URLs & Remove words and digits contain digits
    def remove_urls(self,df,feature_names):
        df = df.copy()
        for feature in feature_names:
            df[feature] = df[feature].apply(lambda x :  re.sub(r'^https?:\/\/.*[\r\n]*', '', x))
        return df

    def remove_words_with_digits(self,df, feature_names):
        df = df.copy()
        for feature in feature_names:
            df[feature] = df[feature].apply(lambda x : " ".join(s for s in x.split() if not any(c.isdigit() for c in s)))
        return df

    # Remove Stopwords
    def remove_stopwords(self,df,feature_names):
        df = df.copy()
        eng_stopwords = set(stopwords.words('english'))
        for feature in feature_names:
            df[feature] = df[feature].apply(lambda text: " ".join(word for word in text.split() if not word in eng_stopwords))
        return df

    # Tokenization
    def tokenize_and_normalization(self,df,feature_names):
        lemmatizer=WordNetLemmatizer()
        for feature in feature_names:
            df[feature] = df[feature].apply(lambda x :" ".join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))

        return df


    def fit(self,df):
        return self

    def transform(self,df):
        df =df.copy()
        #removing non utf8 words
        df = self.remove_non_utf8_words(df,self.feature_names)
        #expanding contractions
        df = self.expand_contractions(df,self.feature_names)
        #changing all to lower case
        df = self.change_to_lower_case(df,self.feature_names)
        #remvoing punctuations
        df = self.remove_punctuations(df,self.feature_names)
        #removing urls
        df = self.remove_urls(df,self.feature_names)
        #removing words with digits
        df = self.remove_words_with_digits(df,self.feature_names)
        #remove stopwords
        df = self.remove_stopwords(df,self.feature_names)
        #remove tokenize and normalization
        df = self.tokenize_and_normalization(df,self.feature_names)


        return df

# Vectorizing Text
class CustomTfidVectorizer(BaseEstimator,TransformerMixin):
    def __init__(self,feature_name,max_features = None):
        self.max_features = max_features
        self.feature_name = feature_name
        return None

    def fit(self,df):
        self.TfidVectorizer = TfidfVectorizer(max_features= self.max_features)
        self.TfidVectorizer.fit(df[self.feature_name])
        return self

    def transform(self,df):
        df = df.copy()
        vectors = self.TfidVectorizer.transform(df[self.feature_name]).toarray()
        df[self.TfidVectorizer.get_feature_names()] = vectors
        df.drop(self.feature_name,axis = 1,inplace = True)
        return df

"""##### Which text vectorization technique have you used and why?

Term frequency-inverse document frequency ( TF-IDF) gives a measure that takes the importance of a word into consideration depending on how frequently it occurs in a document and a corpus.Not like Bag of words and Count Vector technique which treats all words equally,TF-IDF can distinguish very common words or rare words

# ***6. ML Model Implementation***
"""

#using pipeline to transform our text data
text_feature_pipeline = Pipeline([
    ('text_preprocessing',CustomTextDataPreprocessing(feature_names=['description'])),
    ('vectorization',CustomTfidVectorizer(feature_name='description',max_features=400))
])

description_feature_vector = text_feature_pipeline.fit_transform(df).iloc[:,11:]

#printing the shapes of our data
print("Train data: ",description_feature_vector.shape)

"""### ML Model - 1"""

SEED = 10

# ML Model - 1 Implementation
#finding optimal number of clusters using the elbow method
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
ssd= []

#Using for loop for iterations from 1 to 30.
for cluster in range(2, 15):
    kmeans = KMeans(n_clusters=cluster,random_state= SEED)
    kmeans.fit(description_feature_vector)
    preds = kmeans.predict(description_feature_vector)
    score = silhouette_score(description_feature_vector, preds)
    print("For n_clusters = {}, Silhouette score is {}".format(cluster, score))
    ssd.append(kmeans.inertia_)

"""#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."""

# Visualizing evaluation Metric Score chart
plt.plot(range(2, 15), ssd)
plt.xticks(range(2,15))
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('Sum of Squared distance')
plt.show()

"""From the above Elbow method and silhouette score I am choosing 12 would be the perfect number of clusters for this problem. So I am finally training kmeans with 12 clusters."""

#training the K-means model on a dataset
kmeans = KMeans(n_clusters= 12, init='k-means++', random_state= SEED)
y_predict= kmeans.fit_predict(description_feature_vector)

#Predict the clusters and evaluate the silhouette score
score = silhouette_score(description_feature_vector, y_predict)
print("Silhouette score is {}".format(score))

#Reducing the number of features to visualize it in 2D or 3D plot
pca = PCA(n_components = 3)
X = pca.fit_transform(description_feature_vector)

plt.figure(figsize=(15,10))
plt.title('customer segmentation based on Recency and Monetary')
plt.scatter(X[:, 0], X[:, 1], c=y_predict, s=50)

centers = kmeans.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.5)
plt.show()

#plotting 3D Graph
from mpl_toolkits.mplot3d import Axes3D
fig=plt.figure(figsize=(15,10))
plt.title('3d visualization of Recency Frequency and Monetary')
ax=fig.add_subplot(111,projection='3d')
xs=X[:,0]
ys=X[:,1]
zs=X[:,2]
ax.scatter(xs,ys,zs,s=5,c = y_predict)
ax.set_xlabel('PCA 0')
ax.set_ylabel('PCA 1')
ax.set_zlabel('PCA 2')
plt.show()

"""At the end, from the 2D and 3D plot we can say that K-means done not a bad job, but we cannot get into a conclusion by plotting this PCA features.

### ML Model - 2
"""

# Using the dendogram to find the optimal number of clusters
import scipy.cluster.hierarchy as sch
plt.figure(figsize=(13,8))
dendrogram = sch.dendrogram(sch.linkage(description_feature_vector, method = 'ward'))
plt.title('Dendrogram')
plt.xlabel('Customers')
plt.ylabel('Euclidean Distances')
plt.show() # find largest vertical distance we can make without crossing any other horizontal line

# ML Model - 2 Implementation

from sklearn.cluster import AgglomerativeClustering
aggh = AgglomerativeClustering(n_clusters= 9, affinity='euclidean', linkage='ward')
aggh.fit(description_feature_vector)
#Predicting using our model
y_hc=aggh.fit_predict(description_feature_vector)

#Silhouette Coefficient
print("Silhouette Coefficient: %0.3f"%silhouette_score(description_feature_vector,y_hc, metric='euclidean'))

"""### ML Model - 3"""

#model 3

#finding optimal number of clusters using the elbow method
from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score

eps_range=range(6,12) # note, we will scale this down by 100 as we want to explore 0.06 - 0.11 range
minpts_range=range(5,14)

silhouette_scores= []
comb = []

#Using for loop for iterations from 1 to 30.
for k in eps_range:
    for j in minpts_range:
        # Set the model and its parameters+
        model = DBSCAN(eps=k/100, min_samples=j)
        # Fit the model
        clm = model.fit(description_feature_vector)
        # Calculate Silhoutte Score and append to a list
        silhouette_scores.append(silhouette_score(description_feature_vector, clm.labels_, metric='euclidean'))
        comb.append(str(k)+"|"+str(j)) # axis values for the graph

# Plot the resulting Silhouette scores on a graph
plt.figure(figsize=(20,5))
plt.plot(comb, silhouette_scores, 'bo-')
plt.xlabel('Epsilon/100 | MinPts')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Score based on different combnation of Hyperparameters')
plt.show()

"""Looking at the above, we can see that eps=0.08 produce the highest scores.
A few combinations ended up having very similar scores, indicating that the clustering output for those combinations would also be similar.
"""

final_model = DBSCAN(eps=6/100, min_samples=5)
clm = model.fit(description_feature_vector)
print(silhouette_score(description_feature_vector, clm.labels_, metric='euclidean'))

plt.scatter(X[:,0], X[:,1], c= clm.labels_)

"""### 1. Which Evaluation metrics did you consider for a positive business impact and why?

Silhouette score helped us to find the best model. And helped us to choose the best model among these.

### 2. Which ML model did you choose from the above created models as your final prediction model and why?

K-Means did better job in this dataset. It gives the insight that 12 is the perfect number of cluster, but Incase of Aggloramative Clustering  we saw that the 9 is the optimal cluster.Might be 9 is better, but I chose 12 in K-means, I got  a better result with that using k-means. And DBSCAN doen't perform well in this dataset.

# ***7.*** ***Saving Best Model***
"""

!pip install streamlit

import streamlit as st

# Display clustered movies
st.title('Movie Clustering')

for cluster_id in range(k):
    st.subheader(f'Cluster {cluster_id + 1}')
    cluster_movies = df[df['cluster'] == cluster_id]['title'].tolist()
    for movie in cluster_movies:
        st.write(movie)

# Create a Streamlit web page
st.title('Netflix Movies Clustering')

# Display movies by cluster
unique_clusters = sorted(list(set(y_predict)))
for cluster in unique_clusters:
    cluster_movies = df[y_predict == cluster]['Title']
    st.write(f'Cluster {cluster + 1}')
    st.write(cluster_movies)